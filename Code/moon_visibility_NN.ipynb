{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Moon Age</th>\n",
       "      <th>Sunset</th>\n",
       "      <th>Moonset</th>\n",
       "      <th>Lag</th>\n",
       "      <th>Moon Alt</th>\n",
       "      <th>Moon Az</th>\n",
       "      <th>Sun Alt</th>\n",
       "      <th>Sun Az</th>\n",
       "      <th>Moon-Earth Dist</th>\n",
       "      <th>Sun-Moon Dist</th>\n",
       "      <th>ARCL</th>\n",
       "      <th>ARCV</th>\n",
       "      <th>DAZ</th>\n",
       "      <th>Illumination</th>\n",
       "      <th>Parallax</th>\n",
       "      <th>Cloud Level</th>\n",
       "      <th>Seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.730922</td>\n",
       "      <td>53.95763</td>\n",
       "      <td>-1.08271</td>\n",
       "      <td>0.722</td>\n",
       "      <td>-0.00835</td>\n",
       "      <td>0.01044</td>\n",
       "      <td>27.05935</td>\n",
       "      <td>1.51838</td>\n",
       "      <td>252.00232</td>\n",
       "      <td>-1.73173</td>\n",
       "      <td>259.02584</td>\n",
       "      <td>0.00271</td>\n",
       "      <td>0.99508</td>\n",
       "      <td>7.73820</td>\n",
       "      <td>3.25011</td>\n",
       "      <td>7.02353</td>\n",
       "      <td>0.00455</td>\n",
       "      <td>54.14174</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Not_seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.733919</td>\n",
       "      <td>53.95763</td>\n",
       "      <td>-1.08271</td>\n",
       "      <td>1.687</td>\n",
       "      <td>-0.01784</td>\n",
       "      <td>0.02230</td>\n",
       "      <td>57.80819</td>\n",
       "      <td>3.36486</td>\n",
       "      <td>235.23797</td>\n",
       "      <td>-3.31808</td>\n",
       "      <td>243.97528</td>\n",
       "      <td>0.00266</td>\n",
       "      <td>0.98725</td>\n",
       "      <td>10.99616</td>\n",
       "      <td>6.68294</td>\n",
       "      <td>8.73731</td>\n",
       "      <td>0.00918</td>\n",
       "      <td>55.04632</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Not_seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.739819</td>\n",
       "      <td>53.95763</td>\n",
       "      <td>-1.08271</td>\n",
       "      <td>0.685</td>\n",
       "      <td>-0.02074</td>\n",
       "      <td>0.02593</td>\n",
       "      <td>67.20869</td>\n",
       "      <td>3.82122</td>\n",
       "      <td>233.19221</td>\n",
       "      <td>-3.52379</td>\n",
       "      <td>236.46999</td>\n",
       "      <td>0.00252</td>\n",
       "      <td>0.98095</td>\n",
       "      <td>8.04228</td>\n",
       "      <td>7.34501</td>\n",
       "      <td>3.27778</td>\n",
       "      <td>0.00492</td>\n",
       "      <td>58.03155</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Not_seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.742824</td>\n",
       "      <td>53.58333</td>\n",
       "      <td>-2.43333</td>\n",
       "      <td>1.738</td>\n",
       "      <td>-0.02922</td>\n",
       "      <td>0.03653</td>\n",
       "      <td>94.68585</td>\n",
       "      <td>6.40548</td>\n",
       "      <td>245.47535</td>\n",
       "      <td>-5.81151</td>\n",
       "      <td>253.06492</td>\n",
       "      <td>0.00244</td>\n",
       "      <td>0.98427</td>\n",
       "      <td>14.37485</td>\n",
       "      <td>12.21699</td>\n",
       "      <td>7.58956</td>\n",
       "      <td>0.01565</td>\n",
       "      <td>59.59324</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Not_seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.742824</td>\n",
       "      <td>52.63860</td>\n",
       "      <td>-1.13169</td>\n",
       "      <td>1.736</td>\n",
       "      <td>-0.02873</td>\n",
       "      <td>0.03592</td>\n",
       "      <td>93.09817</td>\n",
       "      <td>6.46498</td>\n",
       "      <td>246.07108</td>\n",
       "      <td>-5.86126</td>\n",
       "      <td>253.40983</td>\n",
       "      <td>0.00244</td>\n",
       "      <td>0.98427</td>\n",
       "      <td>14.33820</td>\n",
       "      <td>12.32624</td>\n",
       "      <td>7.33875</td>\n",
       "      <td>0.01557</td>\n",
       "      <td>59.58563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.745726</td>\n",
       "      <td>52.63860</td>\n",
       "      <td>-1.13169</td>\n",
       "      <td>0.761</td>\n",
       "      <td>-0.01582</td>\n",
       "      <td>0.01977</td>\n",
       "      <td>51.24933</td>\n",
       "      <td>3.50315</td>\n",
       "      <td>262.44978</td>\n",
       "      <td>-3.43885</td>\n",
       "      <td>267.72094</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.99075</td>\n",
       "      <td>8.71450</td>\n",
       "      <td>6.94200</td>\n",
       "      <td>5.27116</td>\n",
       "      <td>0.00577</td>\n",
       "      <td>60.83454</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Not_seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.745726</td>\n",
       "      <td>51.87967</td>\n",
       "      <td>-0.41748</td>\n",
       "      <td>0.759</td>\n",
       "      <td>-0.01564</td>\n",
       "      <td>0.01956</td>\n",
       "      <td>50.69011</td>\n",
       "      <td>3.53057</td>\n",
       "      <td>262.58959</td>\n",
       "      <td>-3.46038</td>\n",
       "      <td>267.74070</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.99075</td>\n",
       "      <td>8.68183</td>\n",
       "      <td>6.99094</td>\n",
       "      <td>5.15110</td>\n",
       "      <td>0.00573</td>\n",
       "      <td>60.83247</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Not_seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.745726</td>\n",
       "      <td>51.39822</td>\n",
       "      <td>-0.19837</td>\n",
       "      <td>0.759</td>\n",
       "      <td>-0.01555</td>\n",
       "      <td>0.01944</td>\n",
       "      <td>50.38849</td>\n",
       "      <td>3.55198</td>\n",
       "      <td>262.67600</td>\n",
       "      <td>-3.47683</td>\n",
       "      <td>267.75591</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.99075</td>\n",
       "      <td>8.67048</td>\n",
       "      <td>7.02881</td>\n",
       "      <td>5.07991</td>\n",
       "      <td>0.00571</td>\n",
       "      <td>60.83099</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Not_seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.745726</td>\n",
       "      <td>53.95763</td>\n",
       "      <td>-1.08271</td>\n",
       "      <td>0.760</td>\n",
       "      <td>-0.01604</td>\n",
       "      <td>0.02005</td>\n",
       "      <td>51.96781</td>\n",
       "      <td>3.43075</td>\n",
       "      <td>262.20043</td>\n",
       "      <td>-3.37986</td>\n",
       "      <td>267.64850</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.99075</td>\n",
       "      <td>8.71957</td>\n",
       "      <td>6.81061</td>\n",
       "      <td>5.44807</td>\n",
       "      <td>0.00578</td>\n",
       "      <td>60.83909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Not_seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.748732</td>\n",
       "      <td>53.58333</td>\n",
       "      <td>-2.43333</td>\n",
       "      <td>1.816</td>\n",
       "      <td>-0.02947</td>\n",
       "      <td>0.03682</td>\n",
       "      <td>95.45765</td>\n",
       "      <td>6.69411</td>\n",
       "      <td>277.61713</td>\n",
       "      <td>-6.00369</td>\n",
       "      <td>291.47930</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.99931</td>\n",
       "      <td>18.77762</td>\n",
       "      <td>12.69780</td>\n",
       "      <td>13.86218</td>\n",
       "      <td>0.02661</td>\n",
       "      <td>60.77741</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Not_seen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date  Latitude  Longitude  Moon Age   Sunset  Moonset       Lag  \\\n",
       "0  5.730922  53.95763   -1.08271     0.722 -0.00835  0.01044  27.05935   \n",
       "1  5.733919  53.95763   -1.08271     1.687 -0.01784  0.02230  57.80819   \n",
       "2  5.739819  53.95763   -1.08271     0.685 -0.02074  0.02593  67.20869   \n",
       "3  5.742824  53.58333   -2.43333     1.738 -0.02922  0.03653  94.68585   \n",
       "4  5.742824  52.63860   -1.13169     1.736 -0.02873  0.03592  93.09817   \n",
       "5  5.745726  52.63860   -1.13169     0.761 -0.01582  0.01977  51.24933   \n",
       "6  5.745726  51.87967   -0.41748     0.759 -0.01564  0.01956  50.69011   \n",
       "7  5.745726  51.39822   -0.19837     0.759 -0.01555  0.01944  50.38849   \n",
       "8  5.745726  53.95763   -1.08271     0.760 -0.01604  0.02005  51.96781   \n",
       "9  5.748732  53.58333   -2.43333     1.816 -0.02947  0.03682  95.45765   \n",
       "\n",
       "   Moon Alt    Moon Az  Sun Alt     Sun Az  Moon-Earth Dist  Sun-Moon Dist  \\\n",
       "0   1.51838  252.00232 -1.73173  259.02584          0.00271        0.99508   \n",
       "1   3.36486  235.23797 -3.31808  243.97528          0.00266        0.98725   \n",
       "2   3.82122  233.19221 -3.52379  236.46999          0.00252        0.98095   \n",
       "3   6.40548  245.47535 -5.81151  253.06492          0.00244        0.98427   \n",
       "4   6.46498  246.07108 -5.86126  253.40983          0.00244        0.98427   \n",
       "5   3.50315  262.44978 -3.43885  267.72094          0.00240        0.99075   \n",
       "6   3.53057  262.58959 -3.46038  267.74070          0.00240        0.99075   \n",
       "7   3.55198  262.67600 -3.47683  267.75591          0.00240        0.99075   \n",
       "8   3.43075  262.20043 -3.37986  267.64850          0.00240        0.99075   \n",
       "9   6.69411  277.61713 -6.00369  291.47930          0.00240        0.99931   \n",
       "\n",
       "       ARCL      ARCV       DAZ  Illumination  Parallax  Cloud Level      Seen  \n",
       "0   7.73820   3.25011   7.02353       0.00455  54.14174          1.0  Not_seen  \n",
       "1  10.99616   6.68294   8.73731       0.00918  55.04632          0.5  Not_seen  \n",
       "2   8.04228   7.34501   3.27778       0.00492  58.03155          0.5  Not_seen  \n",
       "3  14.37485  12.21699   7.58956       0.01565  59.59324          1.0  Not_seen  \n",
       "4  14.33820  12.32624   7.33875       0.01557  59.58563          0.0      Seen  \n",
       "5   8.71450   6.94200   5.27116       0.00577  60.83454          1.0  Not_seen  \n",
       "6   8.68183   6.99094   5.15110       0.00573  60.83247          0.5  Not_seen  \n",
       "7   8.67048   7.02881   5.07991       0.00571  60.83099          1.0  Not_seen  \n",
       "8   8.71957   6.81061   5.44807       0.00578  60.83909          1.0  Not_seen  \n",
       "9  18.77762  12.69780  13.86218       0.02661  60.77741          1.0  Not_seen  "
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = False\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3626 rows\n",
      "Selected 3091 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "icouk_data_file = '..\\\\Data\\\\icouk_sighting_data_with_params.csv'\n",
    "icop_data_file = '..\\\\Data\\\\icop_ahmed_2020_sighting_data_with_params.csv'\n",
    "alrefay_data_file = '..\\\\Data\\\\alrefay_2018_sighting_data_with_params.csv'\n",
    "allawi_data_file = '..\\\\Data\\\\schaefer_odeh_allawi_2022_sighting_data_with_params.csv'\n",
    "\n",
    "icouk_data = pd.read_csv(icouk_data_file)\n",
    "icop_data = pd.read_csv(icop_data_file)\n",
    "alrefay_data = pd.read_csv(alrefay_data_file)\n",
    "allawi_data = pd.read_csv(allawi_data_file)\n",
    "\n",
    "data = pd.concat([icouk_data,icop_data,alrefay_data,allawi_data])\n",
    "\n",
    "print(f\"Loaded {data.shape[0]} rows\")\n",
    "\n",
    "data = data.drop([\"Index\",\"q\",\"W\",\"q'\",\"W'\",'Visibility'], axis = 1)\n",
    "\n",
    "if METHOD: # method and methods columns, will be changed\n",
    "    data = data.drop('Seen', axis = 1) # replaced by method column\n",
    "    ptype = [r\"Not_seen\", r\"Seen_eye\", r\"Seen_binoculars\", r\"Seen_telescope\", r\"Seen_ccd\"] # CHANGE THIS\n",
    "else:\n",
    "    data = data[data[\"Method\"] !=\"Seen_binoculars\"] #DROP BINOCULARS\n",
    "    data = data[data[\"Method\"] !=\"Seen_ccd\"] #DROP CCD\n",
    "    data = data[data[\"Method\"] !=\"Seen_telescope\"] #DROP TELESCOPE\n",
    "    \n",
    "    data=data.drop(['Method','Methods'], axis = 1)\n",
    "    # List of label options\n",
    "    ptype = [r\"Seen\", r\"Not_seen\"]\n",
    "\n",
    "print(f\"Selected {data.shape[0]} rows\")\n",
    "\n",
    "variable_list =  data.columns.tolist()\n",
    "variable_list.remove('Seen')\n",
    "\n",
    "data['Sunset'] -= data['Date'] # reducing magnitude of date datapoints\n",
    "data['Moonset'] -= data['Date']\n",
    "data['Date'] -= 2400000\n",
    "data['Date'] *= 1/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(data['Seen'])\n",
    "X = np.array(data[variable_list])\n",
    "\n",
    "y[y == 'Seen'] = int(1)\n",
    "y[y == 'Not_seen'] = int(0)\n",
    "\n",
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X, y, test_size=0.3)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class CustomNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)  # Output layer with 1 neuron for binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Sigmoid activation for binary classification, now softmax\n",
    "        return x\n",
    "    \n",
    "input_size = len(variable_list)  # Number of input variables\n",
    "hidden_size = 32  # Number of neurons in the hidden layers #32\n",
    "output_size = 2 # for binary output\n",
    "model = CustomNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define binary cross-entropy loss and Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss() #nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.0001) #0.001\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=2, factor=0.9) # i dont know what this is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MY METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "Y_train_tensor = torch.from_numpy(Y_train.astype(\"float64\")).float()\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "Y_test_tensor = torch.from_numpy(Y_test.astype(\"float64\")).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Paramaters for a loop\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     correct = 0\n",
    "#     for i in range(0, len(X_train), batch_size):\n",
    "#         inputs = X_train_tensor[i:i+batch_size]\n",
    "#         targets = Y_train_tensor[i:i+batch_size]\n",
    "\n",
    "#         targets = targets.type(torch.LongTensor)  \n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "        \n",
    "        \n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#     # Print training loss after each epoch\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANNA METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data prep for anna code\n",
    "trainset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "testset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, optimiser, device):\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, labels) in enumerate(trainloader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        #p_y = F.softmax(model(data), dim=1)\n",
    "        #loss = model.loss(p_y, labels)\n",
    "        p_y = model(data)\n",
    "        loss_criterion = nn.CrossEntropyLoss()\n",
    "        labels = labels.type(torch.LongTensor)  \n",
    "        loss = loss_criterion(p_y, labels)\n",
    "            \n",
    "        train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    train_loss /= len(trainloader.dataset)\n",
    "    return train_loss\n",
    "\n",
    "def test(model, testloader, device):\n",
    "\n",
    "    correct = 0\n",
    "    #total = 0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(testloader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            #p_y = F.softmax(model(data), dim=1)\n",
    "            #loss = model.loss(p_y, labels)\n",
    "            p_y = model(data)\n",
    "            loss_criterion = nn.CrossEntropyLoss()\n",
    "            labels = labels.type(torch.LongTensor)  \n",
    "            loss = loss_criterion(p_y, labels)\n",
    "                \n",
    "            test_loss += loss.item() * data.size(0)\n",
    "\n",
    "            preds = p_y.argmax(dim=1, keepdim=True)\n",
    "            correct += preds.eq(labels.view_as(preds)).sum().item()\n",
    "\n",
    "        test_loss /= len(testloader.dataset)\n",
    "        accuracy = correct / len(testloader.dataset)\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Validation Loss: 0.349864, Validation Accuracy: 0.857759\n",
      "Current learning rate is: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 0.300025, Validation Accuracy: 0.868534\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 2, Validation Loss: 0.386030, Validation Accuracy: 0.834052\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 3, Validation Loss: 0.278281, Validation Accuracy: 0.892241\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 4, Validation Loss: 0.274650, Validation Accuracy: 0.879310\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 5, Validation Loss: 0.291926, Validation Accuracy: 0.872845\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 6, Validation Loss: 0.279655, Validation Accuracy: 0.883621\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 7, Validation Loss: 0.251051, Validation Accuracy: 0.896552\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 8, Validation Loss: 0.267216, Validation Accuracy: 0.894397\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 9, Validation Loss: 0.248770, Validation Accuracy: 0.892241\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 10, Validation Loss: 0.248192, Validation Accuracy: 0.894397\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 11, Validation Loss: 0.295285, Validation Accuracy: 0.879310\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 12, Validation Loss: 0.275668, Validation Accuracy: 0.892241\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 13, Validation Loss: 0.232584, Validation Accuracy: 0.903017\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 14, Validation Loss: 0.232956, Validation Accuracy: 0.898707\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 15, Validation Loss: 0.289405, Validation Accuracy: 0.875000\n",
      "Current learning rate is: 0.001\n",
      "Epoch: 16, Validation Loss: 0.275096, Validation Accuracy: 0.885776\n",
      "Current learning rate is: 0.0009000000000000001\n",
      "Epoch: 17, Validation Loss: 0.227107, Validation Accuracy: 0.903017\n",
      "Current learning rate is: 0.0009000000000000001\n",
      "Epoch: 18, Validation Loss: 0.236456, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.0009000000000000001\n",
      "Epoch: 19, Validation Loss: 0.231263, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.0009000000000000001\n",
      "Epoch: 20, Validation Loss: 0.218930, Validation Accuracy: 0.915948\n",
      "Current learning rate is: 0.0009000000000000001\n",
      "Epoch: 21, Validation Loss: 0.229897, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.0009000000000000001\n",
      "Epoch: 22, Validation Loss: 0.250035, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.0009000000000000001\n",
      "Epoch: 23, Validation Loss: 0.237962, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.0008100000000000001\n",
      "Epoch: 24, Validation Loss: 0.230670, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.0008100000000000001\n",
      "Epoch: 25, Validation Loss: 0.222035, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.0008100000000000001\n",
      "Epoch: 26, Validation Loss: 0.224495, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.000729\n",
      "Epoch: 27, Validation Loss: 0.220185, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.000729\n",
      "Epoch: 28, Validation Loss: 0.232575, Validation Accuracy: 0.915948\n",
      "Current learning rate is: 0.000729\n",
      "Epoch: 29, Validation Loss: 0.263353, Validation Accuracy: 0.898707\n",
      "Current learning rate is: 0.0006561000000000001\n",
      "Epoch: 30, Validation Loss: 0.219237, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.0006561000000000001\n",
      "Epoch: 31, Validation Loss: 0.244303, Validation Accuracy: 0.900862\n",
      "Current learning rate is: 0.0006561000000000001\n",
      "Epoch: 32, Validation Loss: 0.236916, Validation Accuracy: 0.907328\n",
      "Current learning rate is: 0.00059049\n",
      "Epoch: 33, Validation Loss: 0.232551, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.00059049\n",
      "Epoch: 34, Validation Loss: 0.238425, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.00059049\n",
      "Epoch: 35, Validation Loss: 0.219402, Validation Accuracy: 0.903017\n",
      "Current learning rate is: 0.000531441\n",
      "Epoch: 36, Validation Loss: 0.213486, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.000531441\n",
      "Epoch: 37, Validation Loss: 0.210879, Validation Accuracy: 0.915948\n",
      "Current learning rate is: 0.000531441\n",
      "Epoch: 38, Validation Loss: 0.216899, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.000531441\n",
      "Epoch: 39, Validation Loss: 0.222333, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.000531441\n",
      "Epoch: 40, Validation Loss: 0.209048, Validation Accuracy: 0.915948\n",
      "Current learning rate is: 0.000531441\n",
      "Epoch: 41, Validation Loss: 0.223120, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.000531441\n",
      "Epoch: 42, Validation Loss: 0.215788, Validation Accuracy: 0.920259\n",
      "Current learning rate is: 0.000531441\n",
      "Epoch: 43, Validation Loss: 0.219609, Validation Accuracy: 0.915948\n",
      "Current learning rate is: 0.0004782969\n",
      "Epoch: 44, Validation Loss: 0.215451, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.0004782969\n",
      "Epoch: 45, Validation Loss: 0.218339, Validation Accuracy: 0.907328\n",
      "Current learning rate is: 0.0004782969\n",
      "Epoch: 46, Validation Loss: 0.207891, Validation Accuracy: 0.915948\n",
      "Current learning rate is: 0.0004782969\n",
      "Epoch: 47, Validation Loss: 0.213348, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.0004782969\n",
      "Epoch: 48, Validation Loss: 0.219608, Validation Accuracy: 0.903017\n",
      "Current learning rate is: 0.0004782969\n",
      "Epoch: 49, Validation Loss: 0.227108, Validation Accuracy: 0.903017\n",
      "Current learning rate is: 0.00043046721\n",
      "Epoch: 50, Validation Loss: 0.214078, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.00043046721\n",
      "Epoch: 51, Validation Loss: 0.223790, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.00043046721\n",
      "Epoch: 52, Validation Loss: 0.211222, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.000387420489\n",
      "Epoch: 53, Validation Loss: 0.215781, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.000387420489\n",
      "Epoch: 54, Validation Loss: 0.211557, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.000387420489\n",
      "Epoch: 55, Validation Loss: 0.207701, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.000387420489\n",
      "Epoch: 56, Validation Loss: 0.210461, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.000387420489\n",
      "Epoch: 57, Validation Loss: 0.227282, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.000387420489\n",
      "Epoch: 58, Validation Loss: 0.219594, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.0003486784401\n",
      "Epoch: 59, Validation Loss: 0.212446, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.0003486784401\n",
      "Epoch: 60, Validation Loss: 0.207196, Validation Accuracy: 0.915948\n",
      "Current learning rate is: 0.0003486784401\n",
      "Epoch: 61, Validation Loss: 0.212461, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.0003486784401\n",
      "Epoch: 62, Validation Loss: 0.216457, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.0003486784401\n",
      "Epoch: 63, Validation Loss: 0.210849, Validation Accuracy: 0.915948\n",
      "Current learning rate is: 0.00031381059609000004\n",
      "Epoch: 64, Validation Loss: 0.226800, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.00031381059609000004\n",
      "Epoch: 65, Validation Loss: 0.217536, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.00031381059609000004\n",
      "Epoch: 66, Validation Loss: 0.213041, Validation Accuracy: 0.905172\n",
      "Current learning rate is: 0.00028242953648100003\n",
      "Epoch: 67, Validation Loss: 0.210580, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.00028242953648100003\n",
      "Epoch: 68, Validation Loss: 0.206593, Validation Accuracy: 0.915948\n",
      "Current learning rate is: 0.00028242953648100003\n",
      "Epoch: 69, Validation Loss: 0.223217, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.00028242953648100003\n",
      "Epoch: 70, Validation Loss: 0.206078, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.00028242953648100003\n",
      "Epoch: 71, Validation Loss: 0.206227, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.00028242953648100003\n",
      "Epoch: 72, Validation Loss: 0.203619, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.00028242953648100003\n",
      "Epoch: 73, Validation Loss: 0.209493, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.00028242953648100003\n",
      "Epoch: 74, Validation Loss: 0.213057, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.00028242953648100003\n",
      "Epoch: 75, Validation Loss: 0.213372, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.00025418658283290005\n",
      "Epoch: 76, Validation Loss: 0.206754, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.00025418658283290005\n",
      "Epoch: 77, Validation Loss: 0.211517, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.00025418658283290005\n",
      "Epoch: 78, Validation Loss: 0.216820, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.00022876792454961005\n",
      "Epoch: 79, Validation Loss: 0.204626, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.00022876792454961005\n",
      "Epoch: 80, Validation Loss: 0.228426, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.00022876792454961005\n",
      "Epoch: 81, Validation Loss: 0.210097, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.00020589113209464906\n",
      "Epoch: 82, Validation Loss: 0.215707, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.00020589113209464906\n",
      "Epoch: 83, Validation Loss: 0.212561, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.00020589113209464906\n",
      "Epoch: 84, Validation Loss: 0.212002, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.00018530201888518417\n",
      "Epoch: 85, Validation Loss: 0.205672, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.00018530201888518417\n",
      "Epoch: 86, Validation Loss: 0.210021, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.00018530201888518417\n",
      "Epoch: 87, Validation Loss: 0.205100, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.00016677181699666576\n",
      "Epoch: 88, Validation Loss: 0.219636, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.00016677181699666576\n",
      "Epoch: 89, Validation Loss: 0.211211, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.00016677181699666576\n",
      "Epoch: 90, Validation Loss: 0.206195, Validation Accuracy: 0.907328\n",
      "Current learning rate is: 0.0001500946352969992\n",
      "Epoch: 91, Validation Loss: 0.211715, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.0001500946352969992\n",
      "Epoch: 92, Validation Loss: 0.210247, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.0001500946352969992\n",
      "Epoch: 93, Validation Loss: 0.207130, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.0001350851717672993\n",
      "Epoch: 94, Validation Loss: 0.211446, Validation Accuracy: 0.911638\n",
      "Current learning rate is: 0.0001350851717672993\n",
      "Epoch: 95, Validation Loss: 0.209669, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.0001350851717672993\n",
      "Epoch: 96, Validation Loss: 0.222759, Validation Accuracy: 0.913793\n",
      "Current learning rate is: 0.00012157665459056936\n",
      "Epoch: 97, Validation Loss: 0.206353, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.00012157665459056936\n",
      "Epoch: 98, Validation Loss: 0.206681, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.00012157665459056936\n",
      "Epoch: 99, Validation Loss: 0.207566, Validation Accuracy: 0.909483\n",
      "Current learning rate is: 0.00010941898913151243\n",
      "Final validation error:  9.05172413793104\n"
     ]
    }
   ],
   "source": [
    "# #anna method\n",
    "_bestacc = 0.\n",
    "# Paramaters for a loop\n",
    "\n",
    "quiet = False\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, device)\n",
    "    test_loss, accuracy = test(model, test_loader, device)\n",
    "        \n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    # check early stopping criteria:\n",
    "    #if early_stopping and accuracy>_bestacc:\n",
    "    #    _bestacc = accuracy\n",
    "    #    torch.save(model.state_dict(), modfile)\n",
    "    #    best_acc = accuracy\n",
    "    #    best_epoch = epoch\n",
    "        \n",
    "    # create output row:\n",
    "    _results = [epoch, train_loss, test_loss, accuracy]\n",
    "    \n",
    "            \n",
    "    if not quiet:\n",
    "        print('Epoch: {}, Validation Loss: {:4f}, Validation Accuracy: {:4f}'.format(epoch, test_loss, accuracy))\n",
    "        print('Current learning rate is: {}'.format(optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "print(\"Final validation error: \",100.*(1 - accuracy))\n",
    "#if early_stopping:\n",
    "#    print(\"Best validation error: \",100.*(1 - best_acc),\" @ epoch: \"+str(best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.Tensor(X_test)\n",
    "predictions = model(X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential([\n",
    "#    Dense(32, activation='softmax', input_shape=(18,)),\n",
    "#    Dense(32, activation='softmax'),\n",
    "#    Dense(1, activation='sigmoid'), # refelects catagory\n",
    "#])\n",
    "\n",
    "#model.compile(optimizer='adam',\n",
    "#              loss='binary_crossentropy',\n",
    "#              metrics=['accuracy'])\n",
    "#\n",
    "#hist = model.fit(X_train, Y_train,\n",
    "#          batch_size=32, epochs=100,\n",
    "#          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate(X_test, Y_test)[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
